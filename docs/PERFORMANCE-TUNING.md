# âš¡ Performance Tuning: Make Your Compression Fly

> *"The difference between good and great? About 50 GB/s."*

## Why Performance Matters

Imagine you're copying files. Now imagine doing it **100x faster**. That's what proper tuning can achieve. This guide will take you from "it works" to "it flies."

---

## ğŸšï¸ The Speed vs. Size Tradeoff

Think of compression levels like gears in a car:

| Level | Speed | Compression | Best For |
|:-----:|:-----:|:-----------:|:---------|
| ğŸï¸ **1-3** | *Blazing fast* | Good | Real-time streaming, logs |
| ğŸš— **4-6** | Fast | Better | General purpose |
| ğŸšŒ **7-12** | Moderate | Great | Archival, storage |
| ğŸ¢ **13-22** | Slow | Maximum | Long-term cold storage |

> **Rule of Thumb**: Level 3 gives you 80% of the compression at 5x the speed. Start there!

---

## ğŸ¯ Quick Wins (Do These First!)

### 1. Use Pinned Memory
```cpp
// âŒ Slow: Regular memory
void* buffer = malloc(size);

// âœ… Fast: Pinned memory (2-3x faster transfers!)
void* buffer;
cudaMallocHost(&buffer, size);
```
**What it does**: Pinned memory can be transferred directly to/from the GPU without copying through the CPU first.

### 2. Choose the Right Chunk Size

| Chunk Size | GPU Utilization | When to Use |
|:----------:|:---------------:|:------------|
| 16 KB | 60-70% | Many tiny files |
| **64 KB** | **80-85%** | **Most use cases** â­ |
| 128 KB | 85-90% | Large files |
| 256 KB | 90-95% | Maximum throughput |

### 3. Reuse Your Workspace
```cpp
// âŒ Slow: Allocate every time
for (auto& file : files) {
    void* workspace;
    cudaMalloc(&workspace, size);  // Slow!
    compress(file, workspace);
    cudaFree(workspace);           // Even slower!
}

// âœ… Fast: Allocate once, reuse forever
void* workspace;
cudaMalloc(&workspace, size);  // Once!
for (auto& file : files) {
    compress(file, workspace);     // Just reuse it
}
```

---

## ğŸš€ Advanced Techniques

### Multi-Stream Overlap (The Pipeline Trick)

Imagine a factory where one worker loads materials, another processes them, and a third packages themâ€”all at the same time:

```
Time â†’   â”‚ Stream 1 â”‚ Stream 2 â”‚ Stream 3 â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Step 1   â”‚ Upload   â”‚          â”‚          â”‚
Step 2   â”‚ Compress â”‚ Upload   â”‚          â”‚
Step 3   â”‚ Download â”‚ Compress â”‚ Upload   â”‚
Step 4   â”‚          â”‚ Download â”‚ Compress â”‚
Step 5   â”‚          â”‚          â”‚ Download â”‚
```

**Result**: 3x throughput with no extra hardware!

```cpp
// Create multiple streams
cudaStream_t streams[4];
for (int i = 0; i < 4; i++) {
    cudaStreamCreate(&streams[i]);
}

// Use them in rotation
for (int i = 0; i < num_chunks; i++) {
    int s = i % 4;  // Rotate through streams
    upload(streams[s]);
    compress(streams[s]);
    download(streams[s]);
}
```

### The OpenMP Secret Weapon (60+ GB/s)

This is our **highest-performing pattern**:

```cpp
#pragma omp parallel num_threads(8)
{
    // Each thread: own manager + own stream
    auto manager = cuda_zstd::create_manager(3);
    cudaStream_t stream;
    cudaStreamCreate(&stream);
    
    #pragma omp for
    for (int i = 0; i < num_files; i++) {
        manager->compress(files[i], stream);
    }
}
```

**Why it works**: Multiple CPU threads saturate the GPU with work, hiding all latency.

---

## ğŸ“ˆ Benchmark Your Setup

Run our built-in benchmark:
```bash
./benchmark_batch_throughput
```

**Expected output**:
```
=== ZSTD GPU Batch Performance ===
Parallel Batch 64KB  Ã— 1000:  29.42 GB/s âœ“
Parallel Batch 256KB Ã—  500:  61.91 GB/s âœ“
```

If you're not hitting these numbers, check:
1. Is the GPU being throttled? (Power/thermal limits)
2. Are you using pinned memory?
3. Is another process using the GPU?

---

## ğŸ”§ Environment Variables

| Variable | What It Does | Try This |
|:---------|:-------------|:---------|
| `CUDA_ZSTD_DEBUG_LEVEL=0` | Disable debug output | Faster in production |
| `CUDA_ZSTD_POOL_MAX_SIZE=4G` | Allow larger memory pool | For big workloads |

---

## ğŸ“Š Performance Checklist

Before going to production, verify:

- [ ] Using compression level 3 (unless you need higher ratios)
- [ ] Chunk size is 64KB or larger
- [ ] Using pinned memory for host buffers
- [ ] Workspace is pre-allocated and reused
- [ ] Using multiple streams or OpenMP pattern
- [ ] Debug logging is disabled

---

## ğŸ“ Learn More

- [Batch Processing](BATCH-PROCESSING.md) â€” Process thousands of files at once
- [Memory Pool](MEMORY-POOL-IMPLEMENTATION.md) â€” Optimize GPU memory usage
- [Debugging Guide](DEBUGGING-GUIDE.md) â€” When things don't go as planned

---

*Remember: Measure first, optimize second. Happy compressing! ğŸš€*
